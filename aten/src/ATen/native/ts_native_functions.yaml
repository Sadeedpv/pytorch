backend: Lazy
cpp_namespace: torch::lazy
full_codegen:
  - _adaptive_avg_pool2d
  - _adaptive_avg_pool2d_backward
  - _log_softmax
  - _log_softmax_backward_data
  - _softmax
  - _softmax_backward_data
  - abs
  - add.Tensor
  - addcdiv
  - addcmul
  - addmm
  - arange.start_out
  - all
  - any
  - avg_pool2d
  - avg_pool2d_backward
  - baddbmm
  - bernoulli
  - bernoulli.p
  - binary_cross_entropy
  - binary_cross_entropy_backward
  - bitwise_and.Tensor
  - bitwise_or.Tensor
  - block_diag
  - bmm
  - cat
  - clamp
  - clamp_min
  # clone() *should* show up in the IR.
  # For example, tracing through at::reshape() will insert a clone in cases where the output
  # needs to be cloned first before constructing a view, when the input strides are incompatible.
  # This clone needs to be there in order to faithfully re-execute the view later.
  - clone
  - constant_pad_nd
  - convolution
  - convolution_backward
  - cos
  - cumsum
  - div.Tensor
  - div.Tensor_mode
  - elu
  - elu_backward
  - embedding
  - embedding_dense_backward
  - eq.Scalar
  - eq.Tensor
  - exp
  - flip
  - floor
  - frac
  - gather
  - ge.Scalar
  - ge.Tensor
  - gelu
  - gelu_backward
  - glu
  - glu_backward
  - glu_jvp
  - grid_sampler_2d
  - grid_sampler_2d_backward
  - gt.Scalar
  - gt.Tensor
  - hardsigmoid
  - index_select
  - kl_div_backward
  - l1_loss_backward
  - le.Scalar
  - le.Tensor
  - leaky_relu
  - leaky_relu_backward
  - log
  - log2
  - logdet
  - log_sigmoid_backward
  - log_sigmoid_forward
  - lt.Scalar
  - lt.Tensor
  - masked_fill.Scalar
  - masked_fill.Tensor
  - max
  - max.dim
  - max_pool2d_with_indices
  - max_pool2d_with_indices_backward
  - maximum
  - mean
  - mean.dim
  - min
  - minimum
  - mm
  - mul.Tensor
  - mv
  - native_dropout
  - native_dropout_backward
  - native_layer_norm
  - native_layer_norm_backward
  - ne.Scalar
  - ne.Tensor
  - new_empty_strided
  - neg
  - nll_loss_backward
  - nll_loss_forward
  - nll_loss2d_backward
  - nll_loss2d_forward
  - nonzero
  - norm.ScalarOpt_dim
  - pow.Tensor_Scalar
  - pow.Tensor_Tensor
  - random.functional
  - random.from_functional
  - random.to_functional
  - reciprocal
  - relu
  - remainder.Tensor
  - repeat
  - rsqrt
  - scatter_add
  - sgn
  - sigmoid
  - sigmoid_backward
  - silu
  - smooth_l1_loss
  - smooth_l1_loss_backward
  - softplus
  - softplus_backward
  - sort
  - sqrt
  - stack
  - std
  - std.dim
  - std.correction
  - sub.Tensor
  - sum
  - sum.dim_IntList
  - tanh
  - tanh_backward
  - threshold
  - threshold_backward
  - topk
  - trace
  - tril
  - triu
  - trunc
  - upsample_bilinear2d
  - upsample_bilinear2d_backward
  - upsample_nearest2d
  - upsample_nearest2d_backward
  - zero.functional
  - narrow_copy.SymInt
  - alias_copy
  - as_strided_copy
  - diagonal_copy
  - expand_copy
  - permute_copy
  - _reshape_alias_copy
  - select_copy.int
  - detach_copy
  - slice_copy.Tensor
  #- split_copy.Tensor
  #- split_with_sizes_copy
  #- unbind_copy.int
  - squeeze_copy
  - squeeze_copy.dim
  - t_copy
  - transpose_copy.int
  - unsqueeze_copy
  - view_copy
  - view_copy.dtype
  - unfold_copy
  - select_scatter
  - slice_scatter
  - diagonal_scatter
  - as_strided_scatter
supported:
  - _copy_from
  - _copy_from_and_resize
  - empty.memory_format
  - empty_strided
  - fill_.Scalar
  - native_batch_norm
  - native_batch_norm_backward
  - normal_
  - max_pool3d_with_indices
  - max_pool3d_with_indices_backward
  - _to_copy
  - _unsafe_view
  - lift
autograd:
  - max_pool3d
